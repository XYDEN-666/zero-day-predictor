{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80833516",
   "metadata": {},
   "source": [
    "# Week 3: Static Code Analysis Exploration\n",
    "\n",
    "**Goal:** Test our `code_metrics` module by analyzing a real repository at a specific commit and viewing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import git\n",
    "from IPython.display import display\n",
    "\n",
    "# Because we did `pip install -e .`, Python now knows where to find our modules.\n",
    "# The duplicate import line has been removed.\n",
    "# ... other imports ...\n",
    "from feature_extraction.code_metrics import analyze_repo_at_commit\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the repository we want to analyze\n",
    "REPO_NAME = 'redis'\n",
    "\n",
    "# ---- NEW AND IMPROVED PATH LOGIC ----\n",
    "# Find the project root directory (which contains the 'src', 'data', etc. folders)\n",
    "# This is much more robust than using '..'\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "REPO_PATH = os.path.join(PROJECT_ROOT, 'data', '01_raw', 'repositories', REPO_NAME)\n",
    "# ---- END OF NEW LOGIC ----\n",
    "\n",
    "print(f\"Project Root is: {PROJECT_ROOT}\")\n",
    "print(f\"Using repository at: {REPO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addee1a7",
   "metadata": {},
   "source": [
    "### Step 1: Select a Commit to Analyze\n",
    "\n",
    "Let's grab a commit to simulate looking at the code at a past point in time. We'll pick a relatively recent commit from the `unstable` branch of Redis as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    repo = git.Repo(REPO_PATH)\n",
    "    # Get the latest commit from the 'unstable' branch as an example\n",
    "    target_commit = repo.commit('unstable')\n",
    "    commit_hash = target_commit.hexsha\n",
    "    print(f\"Selected repository: {REPO_NAME}\")\n",
    "    print(f\"Target commit hash: {commit_hash}\")\n",
    "    print(f\"Commit date: {target_commit.committed_datetime}\")\n",
    "    print(f\"Commit message: {target_commit.message.strip()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing repo at {REPO_PATH}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ae7f5",
   "metadata": {},
   "source": [
    "### Step 2: Run the Analysis\n",
    "\n",
    "Now we call our main function from `code_metrics.py`. This will check out the commit, walk through all files, analyze them with `lizard`, and then restore the repo to its original state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a good practice to check if a commit hash was found before proceeding\n",
    "if 'commit_hash' in locals():\n",
    "    file_metrics = analyze_repo_at_commit(REPO_PATH, commit_hash)\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame for easy analysis\n",
    "    df_metrics = pd.DataFrame(file_metrics)\n",
    "\n",
    "    print(f\"\\nAnalysis complete. Found metrics for {len(df_metrics)} files.\")\n",
    "else:\n",
    "    print(\"\\nSkipping analysis because a target commit could not be found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320d31f",
   "metadata": {},
   "source": [
    "### Step 3: Explore the Results\n",
    "\n",
    "Let's look at the data we've generated. We can see basic stats and find the most complex files, which might be candidates for closer inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38f842",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "# This will only run if the analysis was successful\n",
    "if 'df_metrics' in locals() and not df_metrics.empty:\n",
    "    display(df_metrics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3817d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a statistical summary of our metrics\n",
    "if 'df_metrics' in locals() and not df_metrics.empty:\n",
    "    display(df_metrics.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f60e97",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Find the top 10 most complex files in this commit\n",
    "if 'df_metrics' in locals() and not df_metrics.empty:\n",
    "    print(\"\\n--- Top 10 Most Complex Files (by total cyclomatic complexity) ---\")\n",
    "    display(df_metrics.sort_values(by='complexity', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d16645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 largest files by Non-Comment Lines of Code (NLOC)\n",
    "if 'df_metrics' in locals() and not df_metrics.empty:\n",
    "    print(\"\\n--- Top 10 Largest Files (by NLOC) ---\")\n",
    "    display(df_metrics.sort_values(by='nloc', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
